{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrND90JXGR9U"
      },
      "source": [
        "# start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pSp2SRfr8A02"
      },
      "outputs": [],
      "source": [
        "# Import\n",
        "from google.colab import files\n",
        "from re import X\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.utils import resample\n",
        "from sklearn.preprocessing import StandardScaler, Binarizer\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, balanced_accuracy_score, cohen_kappa_score,\n",
        "                             hamming_loss, jaccard_score, roc_auc_score, log_loss, brier_score_loss, precision_recall_curve, roc_curve, auc)\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.pipeline import Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "collapsed": true,
        "id": "bOw7DXnS1VWF",
        "outputId": "d646963f-b49c-45f9-c35f-800319682503"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9e1e4016-d682-48b8-96d2-536b7695c2c3\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9e1e4016-d682-48b8-96d2-536b7695c2c3\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving HB_preprocessed.xlsx to HB_preprocessed.xlsx\n"
          ]
        }
      ],
      "source": [
        "# Upload spreadsheet\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6Xq7fnySCwF"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "V-KEPusXSLqL"
      },
      "outputs": [],
      "source": [
        "df = pd.read_excel(\"New_Data_HB (1).xlsx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_Q3Q_zJAUa8X"
      },
      "outputs": [],
      "source": [
        "# Binary encoding\n",
        "df[\"Adjuvant Systemic Therapy\"] = df[\"Adjuvant Systemic Therapy\"] == \"Yes\"\n",
        "df[\"Cholecystitis\"] = df[\"Cholecystitis\"] == \"Yes\"\n",
        "df[\"Cirrhosis\"] = df[\"Cirrhosis\"] == \"Yes\"\n",
        "df[\"Diabetes\"] = df[\"Diabetes\"] == \"Yes\"\n",
        "df[\"Lymphovascular Invasion\"] = df[\"Lymphovascular Invasion\"] == \"Yes\"\n",
        "df[\"Major Liver Resection\"] = df[\"Major Liver Resection\"] == \"Yes\"\n",
        "df[\"Mutlifocal Disease\"] = df[\"Mutlifocal Disease\"] == \"Yes\"\n",
        "df[\"Neoadjuvant Systemic Therapy\"] = df[\"Neoadjuvant Systemic Therapy\"] == \"Yes\"\n",
        "df[\"Perineural Invasion\"] = df[\"Perineural Invasion\"] == \"Yes\"\n",
        "df[\"Sex\"] = df[\"Sex\"] == \"Male\"\n",
        "df[\"Steatosis\"] = df[\"Steatosis\"] == \"Yes\"\n",
        "df[\"Viral Hepatitis\"] = df[\"Viral Hepatitis\"] == \"Yes\"\n",
        "df[\"Palliative Systemic Therapy\"] = df[\"Palliative Systemic Therapy\"] == \"Yes\"\n",
        "df['BAP1'] = df[\"BAP1\"] == \"Mutant\"\n",
        "df['IDH1'] = df[\"IDH1\"] == \"Mutant\"\n",
        "df['KRAS'] = df[\"KRAS\"] == \"Mutant\"\n",
        "df['TP53'] = df[\"TP53\"] == \"Mutant\"\n",
        "df['SMAD4'] = df[\"SMAD4\"] == \"Mutant\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2QuYX1gEWmwT"
      },
      "outputs": [],
      "source": [
        "# One hot encoding for categorical columns. some contain N/A as a category\n",
        "categorical_cols = [\"Tumor Stage\", \"Sample Type\", \"Subgroup\", \"Resection Margin\", \"Race\", \"Primary Tumor Site\",  \"Node Status \", \"Suspicious Node (imaging)\"]\n",
        "\n",
        "df = pd.get_dummies(df, columns=categorical_cols, drop_first=True, dummy_na=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Wu87Z3n4hKIC"
      },
      "outputs": [],
      "source": [
        "# Scale continuous columns\n",
        "continuous_cols = [\"BMI\", \"Fraction Genome Altered\", \"TMB (nonsynonymous)\", \"Tumor Purity\", \"Age at Diagnosis\"]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Function to scale only the specified columns\n",
        "def scale_selected_columns(df, scaler):\n",
        "    df_scaled = df.copy()\n",
        "    df_scaled[continuous_cols] = scaler.fit_transform(df_scaled[continuous_cols])\n",
        "    return df_scaled\n",
        "\n",
        "df = scale_selected_columns(df, StandardScaler())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "XYOkHlqZhXT_"
      },
      "outputs": [],
      "source": [
        "df = df.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEaIV7U1qv5d"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "JoQUFxtBa9N8",
        "outputId": "df4839df-b16d-46ea-f1ce-490545d322a2"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_a4208095-b2d8-44b7-8fba-4f08c49d77c0\", \"HB_preprocessed.xlsx\", 44263)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Download preprocessed file\n",
        "df.to_excel(\"HB_preprocessed.xlsx\", index=False)\n",
        "files.download(\"HB_preprocessed.xlsx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Of9Y5krX408G"
      },
      "source": [
        "# Descriptives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_FSSFEK44Qy"
      },
      "outputs": [],
      "source": [
        "df = pd.read_excel(\"New_Data_HB (1).xlsx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXCut8S746OI"
      },
      "outputs": [],
      "source": [
        "# Continuous cols\n",
        "columns = [\"MSI Score\", \"Sample ID\", \"BMI\", \"Tumor Purity\",\n",
        "           \"Age at Diagnosis\", \"Fraction Genome Altered\", \"TMB (nonsynonymous)\"]\n",
        "\n",
        "summary_stats = df[columns].describe().T\n",
        "summary_stats = summary_stats.rename(columns={\"25%\": \"Q1\", \"50%\": \"Q2 (Median)\", \"75%\": \"Q3\"})\n",
        "summary_stats = summary_stats[[\"mean\", \"std\", \"min\", \"Q1\", \"Q2 (Median)\", \"Q3\", \"max\"]]\n",
        "summary_stats.to_excel(\"HB_descriptives_cont.xlsx\")\n",
        "files.download(\"HB_descriptives_cont.xlsx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UxCZrg45KjA"
      },
      "outputs": [],
      "source": [
        "# Ordinal\n",
        "def calculate_value_counts_and_percentage(df):\n",
        "    result = []\n",
        "    for col in df.columns:\n",
        "        value_counts = df[col].value_counts()\n",
        "        percentage = (df[col].value_counts(normalize=True) * 100).round(2)\n",
        "\n",
        "        temp_df = pd.DataFrame({\n",
        "            'Feature': col,\n",
        "            'Value': value_counts.index,\n",
        "            'Count': value_counts.values,\n",
        "            'Percentage (%)': percentage.values\n",
        "        })\n",
        "        result.append(temp_df)\n",
        "\n",
        "    # Concatenate all DataFrames in the list\n",
        "    final_df = pd.concat(result, ignore_index=True)\n",
        "\n",
        "    # Save the final DataFrame to a CSV file\n",
        "    #final_df.to_csv(output_filename, index=False)\n",
        "    #print(final_df)\n",
        "\n",
        "    return final_df\n",
        "df = df.drop(columns=[\"MSI Score\", \"Sample ID\", \"BMI\", \"Tumor Purity\", \"Age at Diagnosis\", \"Fraction Genome Altered\", \"TMB (nonsynonymous)\"])\n",
        "result = calculate_value_counts_and_percentage(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_BQD9No5Uo2"
      },
      "outputs": [],
      "source": [
        "result.to_excel(\"HB_descriptives_ordinal.xlsx\")\n",
        "files.download(\"HB_descriptives_ordinal.xlsx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2pk9aYsGdgV"
      },
      "source": [
        "# classification models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkD6SX7L6fCQ"
      },
      "source": [
        "## Function definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHTAxtqe71Bt"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-iWb4i0c6HDE"
      },
      "outputs": [],
      "source": [
        "# Function to find best Hyper Parameters using GridSearchCV\n",
        "def train_model(model, ds):\n",
        "  if ds == \"LVI\":\n",
        "    X = X_train_LVI\n",
        "    y = y_train_LVI\n",
        "  elif ds == \"PNI\":\n",
        "    X = X_train_PNI\n",
        "    y = y_train_PNI\n",
        "  else:\n",
        "    raise ValueError(\"Invalid data type. Use 'LVI' or 'PNI'.\")\n",
        "\n",
        "  dt = GridSearchCV(models[model], param_grid = param_grids[model], scoring='roc_auc', cv=5)\n",
        "  dt.fit(X, y)\n",
        "\n",
        "  output = f\"\"\"\n",
        "  MODEL : {model} {ds}\n",
        "  best cv score: {dt.best_score_}\n",
        "  best cv params: {dt.best_params_}\n",
        "  \"\"\"\n",
        "  print(output)\n",
        "  return dt\n",
        "\n",
        "\n",
        "# Function to evaluate models\n",
        "def analyze_cls_model(model, model_name, metrics_df, data, print_output=False):\n",
        "    if data == \"LVI\":\n",
        "        y_test = y_test_LVI\n",
        "        X_test = X_test_LVI\n",
        "    elif data == \"PNI\":\n",
        "        y_test = y_test_PNI\n",
        "        X_test = X_test_PNI\n",
        "    else:\n",
        "      raise ValueError(\"Invalid data type. Use 'LVI' or 'PNI'.\")\n",
        "\n",
        "\n",
        "    pred = model.predict(X_test)\n",
        "        # Assume `pred` are your predictions and `y_test_cls` are the true class labels\n",
        "    conf_matrix = confusion_matrix(y_test, pred)\n",
        "    TP = conf_matrix[1, 1]  # True Positive\n",
        "    TN = conf_matrix[0, 0]  # True Negative\n",
        "    FP = conf_matrix[0, 1]  # False Positive\n",
        "    FN = conf_matrix[1, 0]  # False Negative\n",
        "\n",
        "    # Sensitivity, hit rate, recall, or true positive rate\n",
        "    sensitivity = TP / (TP + FN)\n",
        "    # Specificity or true negative rate\n",
        "    specificity = TN / (TN + FP)\n",
        "    # Precision or positive predictive value\n",
        "    ppv = TP / (TP + FP)\n",
        "    # Negative predictive value\n",
        "    npv = TN / (TN + FN)\n",
        "    # Fall out or false positive rate\n",
        "    fpr = FP / (FP + TN)\n",
        "    # False negative rate\n",
        "    fnr = FN / (TP + FN)\n",
        "    # True positive rate\n",
        "    tpr = TP / (TP + FN)\n",
        "    # False discovery rate\n",
        "    fdr = FP / (TP + FP)\n",
        "\n",
        "    metrics = {\n",
        "        'Model': model_name,\n",
        "        'Sensitivity': sensitivity,\n",
        "        'Accuracy': accuracy_score(y_test, pred),\n",
        "        'Precision': precision_score(y_test, pred, average='binary'),\n",
        "        \"Specificity\": specificity,\n",
        "        'Recall': recall_score(y_test, pred, average='binary'),\n",
        "        'F1 Score': f1_score(y_test, pred, average='binary'),\n",
        "        'TP': TP,\n",
        "        'TN': TN,\n",
        "        'FP': FP,\n",
        "        'FN': FN,\n",
        "        'PPV': ppv,\n",
        "        'NPV': npv,\n",
        "        'FPR': fpr,\n",
        "        'FNR': fnr,\n",
        "        'TPR': tpr,\n",
        "        'FDR': fdr,\n",
        "        'Balanced Accuracy': balanced_accuracy_score(y_test, pred),\n",
        "        'Cohen Kappa Score': cohen_kappa_score(y_test, pred),\n",
        "        'Hamming Loss': hamming_loss(y_test, pred),\n",
        "        'Jaccard Score': jaccard_score(y_test, pred, average='binary')\n",
        "    }\n",
        "\n",
        "    # Probability-based metrics if applicable\n",
        "    if hasattr(model, 'predict_proba'):\n",
        "        prob_pred = model.predict_proba(X_test)[:, 1]\n",
        "        print(\"1.pred: \", prob_pred)\n",
        "        metrics.update({\n",
        "            'ROC AUC': roc_auc_score(y_test, prob_pred)\n",
        "        })\n",
        "    else :\n",
        "      # If not probability-based, use decision function\n",
        "      prob_pred = model.decision_function(X_test)\n",
        "      print(\"2.pred: \", prob_pred)\n",
        "      metrics.update({\n",
        "          'ROC AUC': roc_auc_score(y_test, prob_pred)\n",
        "      })\n",
        "      print(\"pred: \", prob_pred)\n",
        "\n",
        "    def model_predict_proba(X):\n",
        "        return model.predict_proba(X)\n",
        "\n",
        "    if print_output:\n",
        "        output = f\"\"\"Specificity (TNR): {specificity:.4f}\n",
        "PPV (Positive Predictive Value): {ppv:.4f}\n",
        "NPV (Negative Predictive Value): {npv:.4f}\n",
        "FPR (False Positive Rate): {fpr:.4f}\n",
        "FNR (False Negative Rate): {fnr:.4f}\n",
        "TPR (True Positive Rate): {tpr:.4f}\n",
        "FDR (False Discovery Rate): {fdr:.4f}\"\"\"\n",
        "        print(f\"\\nModel: {model_name}\")\n",
        "        print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
        "        print(f\"Accuracy: {metrics['Accuracy']:.4f}\")\n",
        "        print(f\"Precision: {metrics['Precision']:.4f}\")\n",
        "        print(f\"Recall: {metrics['Recall']:.4f}\")\n",
        "        print(f\"F1 Score: {metrics['F1 Score']:.4f}\")\n",
        "        print(f\"Balanced Accuracy: {metrics['Balanced Accuracy']:.4f}\")\n",
        "        print(f\"Cohen Kappa Score: {metrics['Cohen Kappa Score']:.4f}\")\n",
        "        print(f\"Hamming Loss: {metrics['Hamming Loss']:.4f}\")\n",
        "        print(f\"Jaccard Score: {metrics['Jaccard Score']:.4f}\")\n",
        "        print(output)\n",
        "        if 'ROC AUC' in metrics:\n",
        "            print(f\"ROC AUC: {metrics['ROC AUC']:.4f}\")\n",
        "            print(f\"Log Loss: {metrics['Log Loss']:.4f}\")\n",
        "            print(f\"Brier Score Loss: {metrics['Brier Score Loss']:.4f}\")\n",
        "    print(f\"\\nModel: {model_name}\")\n",
        "    print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
        "    metrics_df = pd.concat([metrics_df, pd.DataFrame([metrics])], ignore_index=True)\n",
        "    return metrics_df\n",
        "\n",
        "\n",
        "# Generate the precision recall curves of a particular dataset.\n",
        "# Used to determine best decision threholds\n",
        "def plot_precision_recall_curve(ds):\n",
        "  if ds == \"LVI\":\n",
        "    X_train = X_train_LVI\n",
        "    y_train = y_train_LVI\n",
        "    X_test = X_test_LVI\n",
        "    y_test = y_test_LVI\n",
        "    models = best_LVI_models\n",
        "  elif ds == \"PNI\":\n",
        "    X_train = X_train_PNI\n",
        "    y_train = y_train_PNI\n",
        "    X_test = X_test_PNI\n",
        "    y_test = y_test_PNI\n",
        "    models = best_PNI_models\n",
        "  else:\n",
        "    raise ValueError(\"Invalid dataset. Use 'LVI' or 'PNI'.\")\n",
        "\n",
        "\n",
        "  for model_name, model in models.items():\n",
        "      if hasattr(model, 'predict_proba'):\n",
        "          prob_pred = model.predict_proba(X_train)[:, 1]\n",
        "          precision, recall, thresholds = precision_recall_curve(y_train, prob_pred)\n",
        "      else:\n",
        "          # Handle models without predict_proba (e.g., SVM without probability=True)\n",
        "          prob_pred = model.decision_function(X_train)\n",
        "          precision, recall, thresholds = precision_recall_curve(y_train, prob_pred)\n",
        "\n",
        "      plt.figure()\n",
        "      plt.plot(recall, precision, marker='.')\n",
        "      plt.xlabel('Recall')\n",
        "      plt.ylabel('Precision')\n",
        "      plt.title(f'Precision-Recall Curve for {model_name}-LVI')\n",
        "      plt.grid(True)\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "# Print the precision, recall and F1 scores of a model in a given range of recall values.\n",
        "# Use this to find a good decision threshold after determining a rough range using the precision recall curves\n",
        "def print_threshold_metrics_ranged(model_name, ds, recall_min,  recall_max):\n",
        "  if ds == \"LVI\":\n",
        "      X_train = X_train_LVI\n",
        "      y_train = y_train_LVI\n",
        "      model = best_LVI_models[model_name]\n",
        "  elif ds == \"PNI\":\n",
        "      X_train = X_train_PNI\n",
        "      y_train = y_train_PNI\n",
        "      model = best_PNI_models[model_name]\n",
        "  else:\n",
        "      raise ValueError(\"Invalid dataset. Use 'LVI' or 'PNI'.\")\n",
        "  if hasattr(model, 'predict_proba'):\n",
        "          prob_pred = model.predict_proba(X_train)[:, 1]\n",
        "          precision, recall, thresholds = precision_recall_curve(y_train, prob_pred)\n",
        "  else:\n",
        "      # Handle models without predict_proba (e.g., SVM without probability=True)\n",
        "      prob_pred = model.decision_function(X_train)\n",
        "      precision, recall, thresholds = precision_recall_curve(y_train, prob_pred)\n",
        "\n",
        "  f1_scores = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "  # Find indices where recall is within the specified range\n",
        "  indices = np.where((recall >= recall_min) & (recall <= recall_max))[0]\n",
        "\n",
        "  print(\"Recall\\tPrecision\\tF1 Score\\tThreshold\")\n",
        "  for i in indices:\n",
        "      print(f\"{recall[i]:.2f}\\t\\t{precision[i]:.2f}\\t\\t{f1_scores[i]:.2f}\\t\\t{thresholds[i]:.4f}\")\n",
        "\n",
        "\n",
        "# Print recall, precision and F1 score of a particular threshold\n",
        "def print_threshold_metrics(model_name, ds, threshold):\n",
        "  if ds == \"LVI\":\n",
        "      X_train = X_train_LVI\n",
        "      y_train = y_train_LVI\n",
        "      model = best_LVI_models[model_name]\n",
        "  elif ds == \"PNI\":\n",
        "      X_train = X_train_PNI\n",
        "      y_train = y_train_PNI\n",
        "      model = best_PNI_models[model_name]\n",
        "  else:\n",
        "      raise ValueError(\"Invalid dataset. Use 'LVI' or 'PNI'.\")\n",
        "\n",
        "\n",
        "  if hasattr(model, 'predict_proba'):\n",
        "          prob_pred = model.predict_proba(X_train_PNI)[:, 1]\n",
        "          precision, recall, thresholds = precision_recall_curve(y_train_PNI, prob_pred)\n",
        "  else:\n",
        "      # Handle models without predict_proba (e.g., SVM without probability=True)\n",
        "      prob_pred = model.decision_function(X_train_PNI)\n",
        "      precision, recall, thresholds = precision_recall_curve(y_train_PNI, prob_pred)\n",
        "\n",
        "  # Find the closest threshold index\n",
        "  closest_index = np.argmin(np.abs(thresholds - threshold))\n",
        "\n",
        "  print(f\"Model: {model_name} {ds}\")\n",
        "  print(f\"Threshold: {thresholds[closest_index]:.4f}\")\n",
        "  print(f\"Precision: {precision[closest_index]:.2f}\")\n",
        "  print(f\"Recall: {recall[closest_index]:.2f}\")\n",
        "  print(f\"F1 Score: {2 * (precision[closest_index] * recall[closest_index]) / (precision[closest_index] + recall[closest_index]):.2f}\")\n",
        "  print(\"-\" * 20)\n",
        "\n",
        "\n",
        "# Function to adjust the thresholds\n",
        "def adjust_threshold(model, ds, threshold):\n",
        "  binarizer = Binarizer(threshold=best_threshold)\n",
        "  if ds == \"LVI\":\n",
        "      best_LVI_models[model] = Pipeline([('binarizer', binarizer), ('model', best_LVI_models[model])])\n",
        "  elif ds == \"PNI\":\n",
        "      best_PNI_models[model] = Pipeline([('binarizer', binarizer), ('model', best_PNI_models[model])])\n",
        "  else:\n",
        "    raise ValueError(\"Invalid dataset. Use 'LVI' or 'PNI'.\")\n",
        "\n",
        "# Function to calculate AUC scores for models\n",
        "def calculate_auc_scores(models, X_test, y_test):\n",
        "    auc_scores = {}\n",
        "    for model_name, model in models.items():\n",
        "        if hasattr(model, 'predict_proba'):\n",
        "          prob_pred = model.predict_proba(X_test)[:, 1]\n",
        "        else:\n",
        "          prob_pred = model.decision_function(X_test)\n",
        "        roc_auc = roc_auc_score(y_test, prob_pred)\n",
        "        auc_scores[model_name] = roc_auc\n",
        "    return auc_scores\n",
        "\n",
        "\n",
        "# Function to plot ROC curves for multiple models\n",
        "def plot_roc_curves(models, X_test, y_test, title):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.title(title)\n",
        "    for model_name, model in models.items():\n",
        "        if hasattr(model, 'predict_proba'):\n",
        "          prob_pred = model.predict_proba(X_test)[:, 1]\n",
        "        else:\n",
        "          prob_pred = model.decision_function(X_test)\n",
        "        fpr, tpr, _ = roc_curve(y_test, prob_pred)\n",
        "        roc_auc = roc_auc_score(y_test, prob_pred)\n",
        "        plt.plot(fpr, tpr, label=f\"{model_name} (AUC = {roc_auc:.4f})\")\n",
        "    plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Function to plot ROC curves of models of a particular ds, each plot contains 3 models. Plots sorted in descending order.\n",
        "def plot_roc_groupsofthree(ds):\n",
        "  if ds == \"LVI\":\n",
        "    models = best_LVI_models\n",
        "    X_test = X_test_LVI\n",
        "    y_test = y_test_LVI\n",
        "  elif ds == \"PNI\":\n",
        "    models = best_PNI_models\n",
        "    X_test = X_test_PNI\n",
        "    y_test = y_test_PNI\n",
        "  else:\n",
        "    raise ValueError(\"Invalid dataset. Use 'LVI' or 'PNI'.\")\n",
        "  # Calculate AUC scores\n",
        "  auc_scores = calculate_auc_scores(models, X_test, y_test)\n",
        "  # Sort models by AUC scores in descending order\n",
        "  sorted_models = dict(sorted(models.items(), key=lambda item: auc_scores[item[0]], reverse=True))\n",
        "  # Split the sorted models into groups of 3\n",
        "  sorted_models_groups = [dict(list(sorted_models.items())[i:i+3]) for i in range(0, len(sorted_models), 3)]\n",
        "  # Plot ROC curves for each group\n",
        "  for i, group in enumerate(sorted_models_groups):\n",
        "      plot_roc_curves(group, X_test, y_test, f\"{ds} - ROC Curves (Group {i+1})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MliNBBok8gFa"
      },
      "outputs": [],
      "source": [
        "# Define the classifiers\n",
        "models = {\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    'MLP': MLPClassifier(max_iter = 5000),\n",
        "    'SVM': SVC(probability=True),\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'Decision Tree': DecisionTreeClassifier(random_state= 125),\n",
        "    'Random Forest': RandomForestClassifier(random_state = 125),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(),\n",
        "    'AdaBoost': AdaBoostClassifier(),\n",
        "    'Gaussian Naive Bayes': GaussianNB()\n",
        "}\n",
        "\n",
        "param_grids = {\n",
        "    'KNN': {\n",
        "        'n_neighbors': [90, 95, 97, 99, 101],  # Centered around best values\n",
        "        'leaf_size': [10, 15, 20, 25, 30],  # Focus on lower values\n",
        "        'p': [1, 2],  # Best params used Manhattan (p=1), but keep 2\n",
        "        'weights': ['uniform', 'distance'],  # Best params used both\n",
        "        'metric': ['manhattan', 'minkowski']  # Euclidean didn't perform well\n",
        "    },\n",
        "    'MLP': {\n",
        "        'activation': ['tanh', 'relu'],  # Best params used tanh\n",
        "        'hidden_layer_sizes': [(10,), (50,), (50, 50), (100,)],  # Best params used (10,) and (50,50)\n",
        "        'learning_rate_init': [0.0001, 0.001, 0.01],  # Added 0.0001 for more fine-tuning\n",
        "        'alpha': [0.0001, 0.001, 0.05],  # Best params used 0.0001 and 0.05\n",
        "        'solver': ['sgd', 'adam'],  # Best params used sgd, but keep adam\n",
        "        'learning_rate': ['adaptive', 'constant']  # Best params used adaptive\n",
        "    },\n",
        "    'SVM': {\n",
        "        'C': [0.05, 0.1, 0.3, 1, 3],  # Best params used 0.1 and 0.3\n",
        "        'kernel': ['linear', 'rbf'],  # Best params used linear, remove poly\n",
        "        'gamma': ['scale', 'auto']  # No extreme gamma values needed\n",
        "    },\n",
        "    'Logistic Regression': {\n",
        "        'C': [0.01, 0.1, 1],  # Best params used 0.01 and 1\n",
        "        'solver': ['liblinear', 'saga'],  # Best params used these\n",
        "        'penalty': ['l1', 'l2']  # Both were used in best params\n",
        "    },\n",
        "    'Decision Tree': {\n",
        "        'max_depth': [3, 5, 10, 15, None],  # Best params used 3, remove extreme values\n",
        "        'min_samples_split': [2, 5, 10, 20],  # Slight refinement\n",
        "        'min_samples_leaf': [2, 4, 8, 10]  # Best params used 8 and 10\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'n_estimators': [50, 100, 150, 200],  # Best params used 150 and 200\n",
        "        'max_depth': [1, 3, 10, None],  # Best params used 1 and 10\n",
        "        'min_samples_split': [2, 10, 20, 40],  # Best params used 2 and 40\n",
        "        'bootstrap': [True, False]  # Added for additional tuning\n",
        "    },\n",
        "    'Gradient Boosting': {\n",
        "        'n_estimators': [20, 50, 100],  # Best params used 50\n",
        "        'learning_rate': [0.01, 0.05, 0.1, 0.2],  # Best params used 0.05 and 0.2\n",
        "        'max_depth': [1, 2, 3]  # Best params used 1 and 2, remove extreme values\n",
        "    },\n",
        "    'AdaBoost': {\n",
        "        'n_estimators': [10, 50, 100],  # Best params used 50 and 100\n",
        "        'learning_rate': [0.1, 0.5, 0.75, 1.0],  # Best params used 1.0 and 0.75\n",
        "        'algorithm': ['SAMME', 'SAMME.R']  # Added for optimization\n",
        "    },\n",
        "    'Gaussian Naive Bayes': {\n",
        "        'var_smoothing': [1e-11, 1e-9, 1e-7, 1e-5]  # Removed extreme values like 1e-50\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nz9SnoC_7ph8"
      },
      "source": [
        "## Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otp5Deyv7kzy"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ff6fJxAfF0of"
      },
      "outputs": [],
      "source": [
        "#copy sheet to df\n",
        "df = pd.read_excel(\"HB_preprocessed.xlsx\")\n",
        "LVI_df = df.drop(columns = \"Perineural Invasion\")\n",
        "PNI_df = df.drop(columns = \"Lymphovascular Invasion\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RpDCgBkJSrJQ"
      },
      "outputs": [],
      "source": [
        "LVI_df = LVI_df.drop(\"Sample ID\", axis = 1)\n",
        "PNI_df = PNI_df.drop(\"Sample ID\", axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MbY6iL8tkN8b"
      },
      "outputs": [],
      "source": [
        "# train test split\n",
        "X_train_LVI, X_test_LVI, y_train_LVI, y_test_LVI = train_test_split(\n",
        "    LVI_df.drop(columns=[\"Lymphovascular Invasion\"]),\n",
        "    LVI_df[\"Lymphovascular Invasion\"],\n",
        "    test_size=0.3,\n",
        "    random_state=123\n",
        ")\n",
        "\n",
        "X_train_PNI, X_test_PNI, y_train_PNI, y_test_PNI = train_test_split(\n",
        "    PNI_df.drop(columns=[\"Perineural Invasion\"]),\n",
        "    PNI_df[\"Perineural Invasion\"],\n",
        "    test_size=0.3,\n",
        "    random_state=42\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "gVI-xoPrQRRu"
      },
      "outputs": [],
      "source": [
        "# Find best hyper parameters and print them to terminal\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "for model_name, _ in models.items():\n",
        "    train_model(model_name, \"LVI\")\n",
        "    train_model(model_name, \"PNI\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "HCvppqt6bHPd"
      },
      "outputs": [],
      "source": [
        "# Define the classification models using the best parameters obtained using GridSearchCV\n",
        "best_LVI_models = {\n",
        "    'KNN': KNeighborsClassifier(leaf_size=10, metric='manhattan', n_neighbors=90, p=1, weights='uniform'),\n",
        "    'MLP': MLPClassifier(activation='tanh', alpha=0.0001, hidden_layer_sizes=(10,), learning_rate='constant', learning_rate_init=0.0001, solver='adam', max_iter=5000, random_state=11),\n",
        "    #'SVM': SVC(probability=True, gamma='scale', kernel='linear', C=0.3, random_state=11),\n",
        "    'Logistic Regression': LogisticRegression(C=0.01, penalty='l2', solver='saga'),\n",
        "    'Decision Tree': DecisionTreeClassifier(random_state=125, max_depth=3, min_samples_leaf=10, min_samples_split=2),\n",
        "    'Random Forest': RandomForestClassifier(random_state=125, bootstrap=True, max_depth=1, min_samples_split=2, n_estimators=200),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(learning_rate=0.2, max_depth=1, n_estimators=50),\n",
        "    'AdaBoost': AdaBoostClassifier(algorithm='SAMME', learning_rate=1.0, n_estimators=50),\n",
        "    'Gaussian Naive Bayes': GaussianNB(var_smoothing=1e-11)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "IrbXKxJl6TBF"
      },
      "outputs": [],
      "source": [
        "best_PNI_models = {\n",
        "    'KNN': KNeighborsClassifier(leaf_size=10, metric='manhattan', n_neighbors=97, p=1, weights='distance'),\n",
        "    #'MLP': MLPClassifier(activation='relu', alpha=0.0001, hidden_layer_sizes=(50, 50), learning_rate='constant', learning_rate_init=0.01, solver='adam', max_iter=5000, random_state=11),\n",
        "    #'SVM': SVC(probability=True, gamma='scale', kernel='rbf', C=3, random_state=11),\n",
        "    'Logistic Regression': LogisticRegression(C=1, penalty='l2', solver='liblinear'),\n",
        "    'Decision Tree': DecisionTreeClassifier(random_state=125, max_depth=3, min_samples_leaf=8, min_samples_split=20),\n",
        "    'Random Forest': RandomForestClassifier(random_state=125, bootstrap=True, max_depth=10, min_samples_split=40, n_estimators=150),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(learning_rate=0.05, max_depth=2, n_estimators=50),\n",
        "    'AdaBoost': AdaBoostClassifier(algorithm='SAMME', learning_rate=1.0, n_estimators=100),\n",
        "    'Gaussian Naive Bayes': GaussianNB(var_smoothing=1e-11)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R462M0dqLdVQ"
      },
      "outputs": [],
      "source": [
        "# Fit models\n",
        "for model_name, model in best_LVI_models.items():\n",
        "    model.fit(X_train_LVI, y_train_LVI)\n",
        "for model_name, model in best_PNI_models.items():\n",
        "    model.fit(X_train_PNI, y_train_PNI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "Nsr4AKM97RdP",
        "outputId": "6043c955-d437-48ac-ebec-f3f366c0f8cd"
      },
      "outputs": [],
      "source": [
        "ds = \"PNI\"\n",
        "plot_precision_recall_curve(ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2xIKRTeCYA9"
      },
      "outputs": [],
      "source": [
        "model_name = \"Gradient Boosting\"\n",
        "ds = \"PNI\"\n",
        "recall_min = 0.7  # Lower bound of recall range\n",
        "recall_max = 0.97 # Upper bound of recall range\n",
        "print_threshold_metrics_ranged(model_name, ds, recall_min, recall_max)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xP40LkinFEfr"
      },
      "outputs": [],
      "source": [
        "model_name = \"Gradient Boosting\"\n",
        "ds = \"LVI\"\n",
        "threshold = 0.5662\n",
        "\n",
        "print_threshold_metrics(model_name, ds, threshold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "84f_OSHduTs1"
      },
      "outputs": [],
      "source": [
        "# Adjust thresholds\n",
        "# repeat for every threshold you want to change\n",
        "best_threshold = 0.5662\n",
        "model = \"Gradient Boosting\"\n",
        "ds = \"LVI\"\n",
        "\n",
        "adjust_threshold(model, ds, best_threshold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3fVk0WteM_fb"
      },
      "outputs": [],
      "source": [
        "# Evaluate models and store metrics in a df\n",
        "metrics_columns = [\n",
        "    'Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC', 'Balanced Accuracy', 'Cohen Kappa Score',\n",
        "    'Hamming Loss', 'Jaccard Score', 'Log Loss', 'Brier Score Loss', 'Specificity', 'Sensitivity',\n",
        "    'PPV', 'NPV', 'FPR', 'FNR', 'TPR', 'FDR','TP','TN', 'FP','FN'\n",
        "]\n",
        "# Initialize the DataFrame\n",
        "metrics_LVI = pd.DataFrame(columns=metrics_columns)\n",
        "metrics_PNI = pd.DataFrame(columns=metrics_columns)\n",
        "\n",
        "#Train and evaluate each model\n",
        "for model_name, model in best_LVI_models.items():\n",
        "    model.fit(X_train_LVI, y_train_LVI)\n",
        "    metrics_LVI = analyze_cls_model(model, model_name, metrics_LVI, \"LVI\")\n",
        "for model_name, model in best_PNI_models.items():\n",
        "    model.fit(X_train_PNI, y_train_PNI)\n",
        "    metrics_PNI = analyze_cls_model(model, model_name, metrics_PNI, \"PNI\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "collapsed": true,
        "id": "__aGKXp0AGrN",
        "outputId": "2156b76b-38f5-4a9b-ff59-33f130a6815d"
      },
      "outputs": [],
      "source": [
        "metrics_LVI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "collapsed": true,
        "id": "5ZMtsgMJCmMd",
        "outputId": "4218312e-c051-4f6c-b799-346c47e11c1a"
      },
      "outputs": [],
      "source": [
        "metrics_PNI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "BKnDKtChAa76",
        "outputId": "c3462208-66c7-4ff3-fc6f-26615f59763c"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_044bce94-f1ad-42e2-ab74-38198493ea79\", \"metrics_LVI.xlsx\", 6602)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Download metrics\n",
        "metrics_LVI.to_excel('metrics_LVI.xlsx', index=False)\n",
        "files.download('metrics_LVI.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "OHOQD1em8i6S",
        "outputId": "a5a18f06-61f7-4b31-b955-f184e2cce0fe"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_ca8e60b8-3f44-4a3f-9ce4-51790ab00b51\", \"metrics_PNI.xlsx\", 6768)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "metrics_PNI.to_excel('metrics_PNI.xlsx', index=False)\n",
        "files.download('metrics_PNI.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-r72tS8weUR",
        "outputId": "5bce2957-c9f3-4a6e-c82a-fdb405d37115"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input layer size: 37\n",
            "Hidden layer sizes: (10,)\n",
            "Number of hidden layers: 1\n"
          ]
        }
      ],
      "source": [
        "# Architecture of SKlearn MLP\n",
        "mlp = best_LVI_models[\"MLP\"]\n",
        "mlp.fit(X_train_PNI, y_train_PNI)\n",
        "\n",
        "hidden_layer_sizes = mlp.hidden_layer_sizes\n",
        "number_of_hidden_layers = len(hidden_layer_sizes)\n",
        "number_of_input_features = X_train_PNI.shape[1]\n",
        "\n",
        "# Print the architecture\n",
        "print(\"Input layer size:\", number_of_input_features)\n",
        "print(\"Hidden layer sizes:\", hidden_layer_sizes)\n",
        "print(\"Number of hidden layers:\", number_of_hidden_layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "KleRjxILuyH9",
        "outputId": "2f8cb7c5-baec-464d-d854-53bc9e901e4b"
      },
      "outputs": [],
      "source": [
        "ds = \"PNI\"\n",
        "plot_roc_groupsofthree(ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5A0qizgAEtI2"
      },
      "source": [
        "## Shap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4s4dPMyZ8qnv"
      },
      "source": [
        "### Function definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xP6fBhzqyhvx"
      },
      "outputs": [],
      "source": [
        "!pip install shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "jUPfwP2KykoU"
      },
      "outputs": [],
      "source": [
        "import shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "dqRfUkpgD-k2"
      },
      "outputs": [],
      "source": [
        "# Shap summary plot\n",
        "def plot_shap(ds, model_name):\n",
        "  # Name mapping for proper name display on plot\n",
        "  #TODO: name mapping new columns\n",
        "  name_mapping = {\n",
        "      'Adjuvant Systemic Therapy': 'Adjuvant Systemic Therapy',\n",
        "      'BMI': 'BMI',\n",
        "      'Cholecystitis': 'Cholecystitis',\n",
        "      'Cirrhosis': 'Cirrhosis',\n",
        "      'Diabetes': 'Diabetes',\n",
        "      'Age': 'Age',\n",
        "      'Fraction Genome Altered': 'Fraction Genome Altered',\n",
        "      'Major Liver Resection': 'Major Liver Resection',\n",
        "      'MSI Score': 'MSI Score',\n",
        "      'Multifocal Disease': 'Multifocal Disease',\n",
        "      'Neoadjuvant Systemic Therapy': 'Neoadjuvant Systemic Therapy',\n",
        "      'Palliative Systemic Therapy': 'Palliative Systemic Therapy',\n",
        "      'Sex': 'Sex',\n",
        "      'Steatosis': 'Steatosis',\n",
        "      'TMB (nonsynonymous)': 'TMB (nonsynonymous)',\n",
        "      'Tumor Purity': 'Tumor Purity',\n",
        "      'Viral Hepatitis': 'Viral Hepatitis',\n",
        "      'Resection Margin_R0': 'Resection Margin: R0',\n",
        "      'Resection Margin_R1': 'Resection Margin: R1',\n",
        "      'Resection Margin_R2': 'Resection Margin: R2',\n",
        "      'Sample Type_Metastasis': 'Sample Type: Metastasis',\n",
        "      'Sample Type_Primary': 'Sample Type: Primary',\n",
        "      'Subgroup_Biphenotypic tumors': 'Subgroup: Biphenotypic tumors',\n",
        "      'Subgroup_EHC': 'Subgroup: EHC',\n",
        "      'Subgroup_GB': 'Subgroup: GB',\n",
        "      'Subgroup_HCC': 'Subgroup: HCC',\n",
        "      'Subgroup_IHC': 'Subgroup: IHC',\n",
        "      'Tumor Stage_Moderately differentiated': 'Tumor Stage: Moderately differentiated',\n",
        "      'Tumor Stage_Poorly differentiated': 'Tumor Stage: Poorly differentiated',\n",
        "      'Tumor Stage_Well differentiated': 'Tumor Stage: Well differentiated',\n",
        "      'Primary Tumor Site_Bile Duct': 'Primary Tumor Site: Bile Duct',\n",
        "      'Primary Tumor Site_Gallbladder': 'Primary Tumor Site: Gallbladder',\n",
        "      'Primary Tumor Site_Liver': 'Primary Tumor Site: Liver',\n",
        "      'Race_Asian': 'Race: Asian',\n",
        "      'Race_Black': 'Race: Black',\n",
        "      'Race_Other': 'Race: Other',\n",
        "      'Race_White': 'Race: White',\n",
        "      # New columns\n",
        "      'Age at Diagnosis': 'Age at Diagnosis',\n",
        "      'Mutlifocal Disease': 'Multifocal Disease',  # corrected typo (Mutlifocal -> Multifocal)\n",
        "      'IDH1': 'IDH1',\n",
        "      'KRAS': 'KRAS',\n",
        "      'TP53': 'TP53',\n",
        "      'BAP1': 'BAP1',\n",
        "      'SMAD4': 'SMAD4',\n",
        "      'Lymphovascular Invasion': 'Lymphovascular Invasion',\n",
        "      'Tumor Stage_Poorly differentiated': 'Tumor Stage: Poorly differentiated',\n",
        "      'Tumor Stage_Well differentiated': 'Tumor Stage: Well differentiated',\n",
        "      'Node Status _Negative': 'Node Status: Negative',\n",
        "      'Node Status _Positive': 'Node Status: Positive',\n",
        "      'Suspicious Node (imaging)_No': 'Suspicious Node (imaging): No',\n",
        "      'Suspicious Node (imaging)_Yes': 'Suspicious Node (imaging): Yes',\n",
        "      'Race_BLACK': 'Race: Black',\n",
        "      'Race_OTHER': 'Race: Other',\n",
        "      'Race_WHITE': 'Race: White',\n",
        "      'Primary Tumor Site_Bile duct': 'Primary Tumor Site: Bile Duct'  # corrected case (Bile duct -> Bile Duct)\n",
        "  }\n",
        "\n",
        "\n",
        "  if ds == \"LVI\":\n",
        "    X_train = X_train_LVI\n",
        "    X_test = X_test_LVI\n",
        "    model = best_LVI_models[model_name]\n",
        "  elif ds == \"PNI\":\n",
        "    X_train = X_train_PNI\n",
        "    X_test = X_test_PNI\n",
        "    model = best_PNI_models[model_name]\n",
        "  else:\n",
        "    raise ValueError(\"Invalid dataset. Use 'LVI' or 'PNI'.\")\n",
        "\n",
        "  X_train = pd.DataFrame(X_train)\n",
        "\n",
        "  # Get feature names and map them\n",
        "  feature_names = X_train.columns\n",
        "  mapped_names = [name_mapping.get(name, name) for name in feature_names]\n",
        "\n",
        "  background_data = shap.sample(X_train, 100)\n",
        "\n",
        "  # Wrap the model's predict_proba method to handle feature names (doens't work for models without predict proba (SVM). uncomment the line)\n",
        "  def model_wrapper(X):\n",
        "      return model.predict_proba(pd.DataFrame(X, columns=feature_names))\n",
        "\n",
        "  explainer = shap.KernelExplainer(model_wrapper, background_data)\n",
        "  shap_values = explainer(X_test)\n",
        "\n",
        "  # Select shap values for the first class (index 0) for all instances\n",
        "  shap_values_class0 = shap_values[:, :, 0]\n",
        "  result = (shap_values_class0, X_test, mapped_names)\n",
        "  # Plot the summary plot for the first class\n",
        "  print(f\"Summary Plot for {model_name} Model {ds}\")\n",
        "  shap.summary_plot(shap_values_class0, X_test, feature_names=mapped_names, max_display=25)\n",
        "  shap.summary_plot(shap_values_class0, X_test, feature_names=mapped_names, max_display=25, plot_type=\"bar\")\n",
        "  shap_values_expl = shap.Explanation(shap_values_class0[0], feature_names=mapped_names)\n",
        "  shap.plots.bar(shap_values_expl, max_display=25)\n",
        "  return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zrkr8mXP8vZq"
      },
      "source": [
        "### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "bbc69fa0667d4561ba1b7f348fa1b3b1",
            "f9ad3a32e243467c81fda489eb2fd9bc",
            "167001df48b9470ba231a1bef2c30861",
            "ade5fe807ae84821b2d4f8a4a70e97eb",
            "62ef3a759f3543a7929277db2af93e18",
            "167cd34e573247a4b0612f56d0bd9da0",
            "6770c22ff3594c7c92a86b17e25bcbb7",
            "1f22488d3abe4ff1afe8a76499598955",
            "8de5ec4f9a1143589ca0eda3a05a33d7",
            "e2ca70b7e94a42bcb1d7b43fff11185e",
            "c3f79b6767d94edf8dad3b96f82c1f08",
            "f38d18e436db4eb6a0f4d82dd90945b5",
            "489ab708232a4190b72148ea877b618b",
            "04c8aeab32df4cd1ab8e15d8f36ec312",
            "855fd1db02b2483991c92c6bcd241794",
            "e54b7d466c23413395fc0068b6ef298c",
            "d8849c1e09d14d24bd6636cc92f605d6",
            "66e34224e0ad4ed39e5644035f93219f",
            "54c22b58e4ab42859ff74105a4392816",
            "d17a69acefdb4bc3b1bc299085ebb3d7",
            "ffb9719e266a4f409ab7f1590cb7955d",
            "22446861322b40b2b22a089a4aaa4f79",
            "2a8ba74edfed461b988d2892d6c1948b",
            "0a8c8c13d5364c54a521754b4cf451ae",
            "57c386652bde4d91a6da4aad9c6bfa0e",
            "c8e1ce8de5074f72bac13aa4924d0d13",
            "ded17a97b55f44e8b1aa54e0b4c09258",
            "29cf465ee46a46d482d383776c39e479",
            "4482ec19773d43b09797f9d553847d92",
            "d3d7d0b3ff194c65b1340b54f6c59259",
            "5708a516858c417ebcfa648dd65ea4ff",
            "fbbf49e420784236864f4594165411c3",
            "bff23796f38945c3813395b312e10038",
            "f0df7f7ab91b41629458f13b85439a7b",
            "8298064b1ee5438182c1646001429194",
            "0a79e20c20024ec2889f062f1516562c",
            "ab8377e7e73a41e1b75d7b8df5ff9aa0",
            "7f24a84d10394cf88e3d9a069ca62ed5",
            "764d9df1d5f9420eaa127bad867fe4c6",
            "e6c1cf8090b24e498adfe4c1b1424a4d",
            "6350c6176cde4058bd2d8f456d4156be",
            "c688f1eb961b4a339f8ea5bbf6d197c6",
            "deeda72f98ae4f1cb010a5966d3eb64f",
            "f603f4410f3240138f45337d5ab1603a",
            "b655d16faf484e33b22ae94d211d9c41",
            "03d758a038a7440cb4a52ff02bfd564c",
            "09d3b4a76df24cc883801df67ecb1422",
            "9701a8d90b2848ffbf4a2e6deed2c1ac",
            "fc3a3fc764a84b7b8d3fe30dbd6c1631",
            "d9ddf92ddbb748348d37f41c133525db",
            "f47dacaa0cc746608b211634b77553b4",
            "d40cce39221d4078a7f4c23e8f31619d",
            "5a201539d4c44523989d133c43858a52",
            "0f9b55d53a64434e8062bd80b0b30398",
            "9f0ec5e7891d446f9c07d55b66b42999",
            "bc47f9b6fa0045629e6ea32d8c295812",
            "cdeb0dd6585b4c6ca2b53caaac85ccff",
            "86dd5c72dd9d4128a8b4fbb27d2a54a1",
            "51f502ab65874d68bb7d51fb1c944c22",
            "8385b6efd4c6462b9553e54a594ed91f",
            "798cf33519884deeae7f3907861b18cd",
            "126c72b3b3294f8a928f790bbb63b37c",
            "f25be51814ea4e8a8e0a8fac83d3f402",
            "b83a066e66c34e3d92873a6c4e786357",
            "710a2c25ba3249af93773a470d6fb3b0",
            "c7cd20ab5bbd4461b1adb60f9b83aabc",
            "dfbf9f5622ff4c4fbb00ee298dc710d8",
            "ee93d4fb9d5c4d6db1caf8b727a77891",
            "33cf0202541d4e1e844e28a1244c75ac",
            "c7910bda5cc142eb8f812e552569c13f",
            "823e8d4c1b014881bd1fcc4757740f68",
            "8d5be50f73a84b958314791f069af0c0",
            "35324ea969544857b64cd77f3cfb4166",
            "d7666c35707d47f084c7720a4e119a11",
            "4505678be6fe4703a2397a12a6b5e2bf",
            "de16b981a54142acbf76fe6582734e86",
            "402c84345ba94c8ea67e676f9ec7a7af",
            "c3d12173ac7043158c9891a2edafa4fa",
            "1c77f9392e3d48a0bfd55a232ad7fad7",
            "6b24c2dd6f3b4974be662b4db014539e",
            "d2f266596d94446cb6c643589227534d",
            "8170d564fb3b417ca75b013d42f54568",
            "73a34748d1b44995ab48a3ce32eb4f76",
            "61892c08d40c443c8f84d788604ebbfd",
            "26b14e9305e940f8abf82befbde47d50",
            "256db166c65647dd824b73f0af5bb1d6",
            "05c8428e993e45c593c31201f2c4a1e9",
            "f60a449222224755b3f16d4c4dbb46e0",
            "b13817b1e0604618a3c834054cb77d56",
            "f764e3c57e8242dd97434d6a62607cd9",
            "560fc5b0f55747129fd3c81b05b719dc",
            "ba8e1e730c444752b138d4721347a7e8",
            "6b23f898184d4626a91387b840c9bf61",
            "bbdfc5e205084d198ebccc1633a78326",
            "5012c6b3b1294b37a5e252e2b2b3e84a",
            "674478d7b71349558fc19b31c1df8f47",
            "a0c5f235cd6048d08254dc6a354dcadd",
            "a3cd78610ce14ecebc50edd8bab024e5",
            "0f72fe5a8182453f8b1ea6e1c64831cd",
            "cac26243b8b64423bad608e54a5d9abf",
            "e85cc19a8e5c4531afa1f4963b8d9ac2",
            "eaf53af5168a4add93997bb9b94716f2",
            "4245a1e9688640159608379dce756c16",
            "604dff316e964c8093c1ff34f88fe080",
            "c6a5880e27f54f769889f3ab9c0fe05c",
            "58f06a398d0e431d8780d21e4b7f397c",
            "dc32e67675b849ed8fbbe7c02718e9fb",
            "85fba54033044482bc8b2462a6646881",
            "517e06bd26b345e384c64aa6e27f2755",
            "766c196c5b294143a0ed307ec9f9044b",
            "00798e264e5f43368eb4b30eb26783da",
            "058272c9754b4b5993a1ed328ee778c1",
            "7250927223bf4c0a90381c35c6105c2e",
            "611ca5f916f24eafa96e6ca77ce30136",
            "0d6fdd2bf2354995923747252eb7bf57",
            "f297d4580b354812b790398f068eff09",
            "a0008eab6f164ef1b176b94c5db10e87",
            "129b4e5b94924fb38af77fa4d830d89b",
            "c69a57301440453e81fa8dfaf8a11f17",
            "574bfd5409e04b02a494aa4474fa6339",
            "a0647072abc44b248e09740f4586dcb2",
            "12105db7d5e04000b17c72c9953ed524",
            "6b2421dcd447454f82bad617be793df5",
            "6ea03420b5d842418a88b7a5fbfac039",
            "7a8088b8e2264a748d4a40961ec2a3fc",
            "1d3d8ef957f549ee9c4b54e3f97a0078",
            "0551814172fb45e5a0198d8cd4235d8b",
            "c296cbb0097643e782d7355a0619355e",
            "041efaac997d494abdf360ebb8350405",
            "bddefa57beee4d418f6153f6897311db",
            "b88176bf68a649a483e189d6cba73880",
            "00896e2582074028b03de495e4020b1c",
            "43891ba247e24f16a5af18fb2c3c7c21",
            "328a3e260461401ab0bc85043e42a441",
            "df076910b83f4b0caaf46d2e70969614",
            "61fffcedf16543de911449c51d193c7b",
            "c71b117637ca48b28f37d0acc722ef0a",
            "380f2b18d5b84ec0abd3d349deba34c9",
            "b21ace5af6f2478e9c354251ae9806e7",
            "e0607b5c394446faaf4c1b404f5382b1",
            "ba94d690c0ee41779b720fe065c1627c",
            "cbdcc44e19594eb3ad8c7c6ad1985b43",
            "1839457408e5461a8c38a9d8c262e52e",
            "9b008e5ecce34740b1412b8df1193bd3",
            "da53bc7e4cde4be8933dbbf374b72dbc",
            "e5696adc22594395ba636eefc9455f89",
            "6c7ef474e2dd4c71a83c300a9e820a10",
            "389c9446b49e436a88536e89f6a462cf",
            "365e1a914cb54e9faf57cc1ecf1df17d",
            "8478cf29d9684f9286a69afd1851ce79",
            "011d5b2f2f964a4cb63ee5d5b2a4e826",
            "94caa0f995e24da4b762e1f115fd153f",
            "b17132043172476a9bd1f2037bdd4000",
            "47c13319dedc49fdade6ad1a6444c5e8",
            "f18ceddc980449ca86becc231b1b8955",
            "cb05bd26ad5348f08dfa54fa1f62fc37",
            "7d32671e6e1547929268bcd5426f8236",
            "b28a1b50694643e5a4985ff2d92bc359",
            "b115523f608e4de2a9a7c8c70ec6776f",
            "4824ec8ccbb44cd582fa9cdcc89c1376",
            "882a62a3c6b34924a136362c81b8f79d",
            "c36136f790b54d5dbac0e23aeab435f3",
            "60813bd48d814b2b9948ded2b4693b7c",
            "5abe7f1e34d649b58701453a9923aa13",
            "03a3e541501e4ef380f89d162ae6d89f"
          ]
        },
        "collapsed": true,
        "id": "VQJ4t3JOPhKn",
        "outputId": "07d6c29a-6bdc-4adb-ce15-eff44334f839"
      },
      "outputs": [],
      "source": [
        "print(\"-\" * 100)\n",
        "print(\"PNI\")\n",
        "print(\"-\" * 100)\n",
        "shap_vals_PNI = {}\n",
        "shap_vals_LVI = {}\n",
        "for name, _ in best_PNI_models.items():\n",
        "  shap_vals_PNI[name] = plot_shap(\"PNI\", name)\n",
        "print(\"-\" * 100)\n",
        "print(\"LVI\")\n",
        "print(\"-\" * 100)\n",
        "for name , _ in best_LVI_models.items():\n",
        "  shap_vals_LVI[name] = plot_shap(\"LVI\", name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXp4MDoj8Q0M"
      },
      "source": [
        "# TF MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4mAoHql6zibG"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers, Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.regularizers import l2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "sNzqHyUDHRHc"
      },
      "outputs": [],
      "source": [
        "# Function to evaluate the TF mlp model\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    y_pred_probs = model.predict(X_test).ravel()\n",
        "    y_pred = (y_pred_probs >= 0.5).astype(int)\n",
        "\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)  # Sensitivity\n",
        "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "    specificity = tn / (tn + fp)\n",
        "\n",
        "    # Compute ROC curve and AUC\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_pred_probs)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    # Print metrics\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall (Sensitivity): {recall:.4f}\")\n",
        "    print(f\"Specificity: {specificity:.4f}\")\n",
        "    print(f\"AUC-ROC: {roc_auc:.4f}\")\n",
        "    print(f\"Confusion Matrix: TN={tn}, FP={fp}, FN={fn}, TP={tp}\")\n",
        "\n",
        "    # Plot ROC curve\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(5, 4))\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSsEnln6HM6R"
      },
      "outputs": [],
      "source": [
        "# PNI_MLP\n",
        "PNI_MLP = keras.Sequential([\n",
        "    layers.Dense(78, input_shape=(37,), activation='relu', name='dense_17'),\n",
        "    layers.Dense(39, activation='relu', name='dense_18'),\n",
        "    layers.Dense(20, activation='relu', name='dense_19'),\n",
        "    layers.Dense(10, activation='relu', name='dense_20'),\n",
        "    layers.Dense(1, activation='sigmoid', name='dense_21')\n",
        "])\n",
        "\n",
        "PNI_MLP.layers[0] = layers.Dense(78, input_shape=(78,), activation='relu', name='dense_17', kernel_initializer='glorot_uniform', bias_initializer='zeros')\n",
        "PNI_MLP.compile(optimizer=keras.optimizers.Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "PNI_MLP.summary()\n",
        "PNI_MLP.fit(X_train_PNI, y_train_PNI)\n",
        "evaluate_model(PNI_MLP, X_test_PNI, y_test_PNI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8Lr8c0Pl2b2O",
        "outputId": "7ea37198-8f35-4084-d7f4-7b84915b85dd"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "LVI_MLP = Sequential([\n",
        "    layers.Dropout(0.01, input_shape=(37,), name='dropout_14'),  # Increased dropout for regularization\n",
        "    layers.Dense(64, activation='relu', kernel_regularizer=l2(0.01), name='dense_116'),  # Reduced units and added L2 regularization\n",
        "    layers.BatchNormalization(name='batch_norm_1'),  # Added batch normalization\n",
        "    layers.Dropout(0.3, name='dropout_15'),  # Added another dropout layer\n",
        "    layers.Dense(32, activation='relu', kernel_regularizer=l2(0.01), name='dense_117'),  # Further reduced units\n",
        "    layers.BatchNormalization(name='batch_norm_2'),  # Added batch normalization\n",
        "    layers.Dropout(0.3, name='dropout_16'),  # Added another dropout layer\n",
        "    layers.Dense(16, activation='relu', kernel_regularizer=l2(0.01), name='dense_118'),  # Further reduced units\n",
        "    layers.BatchNormalization(name='batch_norm_3'),  # Added batch normalization\n",
        "    layers.Dropout(0.3, name='dropout_17'),  # Added another dropout layer\n",
        "    layers.Dense(1, activation='sigmoid', name='dense_120')  # Output layer\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "LVI_MLP.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "LVI_MLP.summary()\n",
        "LVI_MLP.fit(X_train_LVI, y_train_LVI)\n",
        "evaluate_model(LVI_MLP, X_test_LVI, y_test_LVI)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "E6Xq7fnySCwF",
        "Of9Y5krX408G",
        "fkD6SX7L6fCQ",
        "4s4dPMyZ8qnv",
        "Zrkr8mXP8vZq"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
